# 신경망에서 딥러닝으로

## 기울기 소실 문제와 활성화 함수

오차 역전파는 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법이다.
가중치를 수정하려면 미분 값, 즉 기울기가 필요하지만 층이 늘어나면서 역전파를 통해 전달되는 이 기울기의 값이 점점 작아서 전달되지 않는 기울기 소실 (vanishing gradient) 문제가 발생하기 시작했다.

<strong>원인은 바로 시그모이드 함수! 시그모이드 함수를 미분했을때 최댓값이 0.3이여서 계속 곱하다보면 계속 0에 가까워져 가중치를 수정하기 어려워지는 것이다.</strong>

### 활성화 함수

![image](https://miro.medium.com/max/666/1*nrxtwp6rzqdFhgYh0x-eVw.png)

## 속도와 정확도 문제를 해결하는 고급 경사 하강법

### 확률적 경사 하강법

경사 하강법은 불필요하게 많은 계산량이 속도를 느리게 할 뿐 아니라, 최적 해를 찾기 전에 최적화 과정이 멈출 수도 있다.

<strong>확률적 경사 하강법(Stochastic Gradient Descent)</strong> 가 이런 단점을 보완한 방법이다.

![image](https://blog.kakaocdn.net/dn/dHK3qz/btqBpQUWCen/GG6J9MDWIX6tZGiG0wvlzk/img.png)

### 모멘텀

모멘텀이란 단어는 '관성, 탄력, 가속도' 라는 뜻이다. 경사 하강법에 탄력을 더해 주는 방법이다. 다시 말해서, 경사 하강법과 마찬가지로 매번 기울기를 구하지만, ㅣㅇ를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향(+, -)을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법이다.

![image](https://thebook.io/img/080228/124.jpg)
